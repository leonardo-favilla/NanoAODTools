{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd081e3",
   "metadata": {},
   "source": [
    "New Version for RUN 3 DATA/MC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e16aaf3-78d5-4a3c-851c-21f771dcf586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.27/01\n",
      "/tmp/x509up_u0 /cvmfs/grid.cern.ch/etc/grid-security/certificates/\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import os\n",
    "from utils.samples import *\n",
    "from utils.variables import *\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from dask.distributed import Client\n",
    "from datetime import datetime\n",
    "ROOT.RDF.Experimental.Distributed.open_files_locally = False\n",
    "\n",
    "os.environ['X509_CERT_DIR'] = \"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "os.environ['X509_USER_PROXY'] = \"/tmp/x509up_u0\"\n",
    "print(os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3271c9-56c5-4ee1-a501-f85177699ab8",
   "metadata": {},
   "source": [
    "Fixed parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f446ef-90d9-46e8-ad16-8ded4fa2fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 1   #34.202 #17.61 #fb #RUN3 eraF\n",
    "cut = requirements # ---> vedi variables.py\n",
    "in_dataset = [\n",
    "    \"DataHT_2018\",\n",
    "    # \"DataHTA_2018\", \n",
    "    # \"DataHTB_2018\", \n",
    "    # \"DataHTC_2018\",\n",
    "    # \"TprimeToTZ_700_2018\", \"TprimeToTZ_1000_2018\", \"TprimeToTZ_1800_2018\",   \n",
    "    # \"QCD_2018\", \n",
    "    # \"TT_2018\", \n",
    "    # \"TT_hadr_2018\", \"TT_semilep_2018\", \"TT_Mtt700to1000_2018\", \"TT_Mtt1000toInf_2018\",\n",
    "    # \"ZJetsToNuNu_2018\", \n",
    "    # \"WJets_2018\",\n",
    "    # \"DataHTC_2022\"\n",
    "             ]\n",
    "recreate_files = True # crea i file di ogni dataset e scrive histos con il nome degli tagli\n",
    "                      # False: appende gli histos al file pre-esistente\n",
    "# run3 = False\n",
    "top_selection = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54586805-5e2c-4b36-a13b-11497d5f2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run3 == True:\n",
    "#     electron_mvaIso = \"Electron_mvaIso_WPL\"\n",
    "# else: \n",
    "    \n",
    "# electron_mvaIso = \"Electron_cutBased\" # mvaFall17V2noIso_WPL no scale factor per UL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29461d93-e531-41df-a8c3-1fd0298ef13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## params cluster\n",
    "\n",
    "sched_port = 23755 #Dask port\n",
    "nmaxpartition = 20 # to set at lower value\n",
    "distributed = True#False#\n",
    "if distributed:\n",
    "    nfiles_max = 999\n",
    "else:\n",
    "    nfiles_max = 5  #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63e6103-21cf-4437-9d7b-2562eeb75059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output folder ---> waiting to implement davix lib\n",
    "if distributed:\n",
    "    folder = \"./results/run2018_benchmark_v2/\"\n",
    "else:\n",
    "    folder = \"./results/run2018_benchmark_v1_singlefile/\"\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "repohisto = folder+\"plots/\"\n",
    "if not os.path.exists(repohisto):\n",
    "    os.mkdir(repohisto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a9be5b-2e52-431b-9709-ee1c3e639b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/distributed/client.py:1128: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | client | scheduler | workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| msgpack | 1.0.3  | 1.0.2     | 1.0.3   |\n",
      "| toolz   | 0.11.2 | 0.11.1    | 0.11.2  |\n",
      "+---------+--------+-----------+---------+\n",
      "Notes: \n",
      "-  msgpack: Variation is ok, as long as everything is above 0.6\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "end of initialization\n"
     ]
    }
   ],
   "source": [
    "# initialization of clusters\n",
    "\n",
    "# upload the proxyfile to the Dask workers to make them able to access data on the grid \n",
    "\n",
    "from distributed.diagnostics.plugin import UploadFile\n",
    "def set_proxy(dask_worker):\n",
    "    import os\n",
    "    import shutil\n",
    "    working_dir = dask_worker.local_directory\n",
    "    proxy_name = 'x509up_u0'\n",
    "    os.environ['X509_USER_PROXY'] = working_dir + '/' + proxy_name\n",
    "    os.environ['X509_CERT_DIR']=\"/cvmfs/grid.cern.ch/etc/grid-security/certificates/\"\n",
    "    shutil.copyfile(working_dir + '/x509up_u0', working_dir + '/../../../x509up_u0')    \n",
    "    os.environ['EXTRA_CLING_ARGS'] = \"-O2\"\n",
    "    return os.environ.get(\"X509_USER_PROXY\"), os.environ.get(\"X509_CERT_DIR\")\n",
    "\n",
    "text_file = open(\"utils/postselection.h\", \"r\")\n",
    "data = text_file.read()\n",
    "def my_initialization_function():\n",
    "    print(ROOT.gInterpreter.ProcessLine(\".O\"))\n",
    "    ROOT.gInterpreter.Declare('{}'.format(data))\n",
    "    print(\"end of initialization\")\n",
    "\n",
    "# set up everything properly\n",
    "if distributed == True:\n",
    "    RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "    client = Client(address=\"tcp://127.0.0.1:\"+str(sched_port))\n",
    "    client.restart()\n",
    "    client.register_worker_plugin(UploadFile(\"/tmp/x509up_u0\"))\n",
    "    client.run(set_proxy)\n",
    "    ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "else:\n",
    "    RDataFrame = ROOT.RDataFrame\n",
    "    my_initialization_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e40aec",
   "metadata": {},
   "source": [
    "Regions definitions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3b1a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all_regions': '', 'compared_regions': 'MET_pt>200'}\n"
     ]
    }
   ],
   "source": [
    "regions_def = regions # ---> vedi variables.py\n",
    "# {\n",
    "#     \"all_regions\" : \"\"\n",
    "#     # \"resolved_1fwjet\": \"EventTopCategory==3 && nForwardJet>0\", \n",
    "#     # \"mixed_1fwjet\": \"EventTopCategory==2 && nForwardJet>0\", \n",
    "#     # \"merged_1fwjet\": \"EventTopCategory==1 && nForwardJet>0\",\n",
    "#     # \"resolved_0fwjet\": \"EventTopCategory==3 && nForwardJet==0\", \n",
    "#     # \"mixed_0fwjet\": \"EventTopCategory==2 && nForwardJet==0\", \n",
    "#     # \"merged_0fwjet\" : \"EventTopCategory==1 && nForwardJet==0\",\n",
    "#     # \"noTopRegion\" : \"EventTopCategory==0\"\n",
    "# }\n",
    "print(regions_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9de3f-0903-4934-9090-15e12c806b20",
   "metadata": {},
   "source": [
    "Importing dict samples with the info for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2fbdbd1-d1d8-4ddf-9d0d-10d6a7b7d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = open(\"utils/dict_samples.json\", \"rb\")\n",
    "samples = json.load(sample_file)\n",
    "sample_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29546c32",
   "metadata": {},
   "source": [
    "INPUT DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06816863-d201-4852-b127-6bd885b566f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DataHT_2018']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d23c148-5df3-45ff-ba4b-1ac967f420de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHT_2018\n"
     ]
    }
   ],
   "source": [
    "if not in_dataset[0] in sample_dict.keys():\n",
    "    datasets = []\n",
    "    print(\"Check the in_dataset string... \", sample_dict.keys())\n",
    "else : \n",
    "    datasets= []\n",
    "    for d in in_dataset:\n",
    "        datasets.append(sample_dict[d])\n",
    "        print(datasets[-1].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "682c7555-c07d-4899-b7d1-12cef3783084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['QCD_2018', 'QCDHT_100to200_2018', 'QCDHT_200to300_2018', 'QCDHT_300to500_2018', 'QCDHT_500to700_2018', 'QCDHT_700to1000_2018', 'QCDHT_1000to1500_2018', 'QCDHT_1500to2000_2018', 'QCDHT_2000toInf_2018', 'ZJetsToNuNu_2018', 'ZJetsToNuNu_HT100to200_2018', 'ZJetsToNuNu_HT200to400_2018', 'ZJetsToNuNu_HT400to600_2018', 'ZJetsToNuNu_HT600to800_2018', 'ZJetsToNuNu_HT800to1200_2018', 'ZJetsToNuNu_HT1200to2500_2018', 'ZJetsToNuNu_HT2500toInf_2018', 'TT_2018', 'TT_hadr_2018', 'TT_semilep_2018', 'TT_Mtt1000toInf_2018', 'TT_Mtt700to1000_2018', 'WJets_2018', 'WJetsHT100to200_2018', 'WJetsHT200to400_2018', 'WJetsHT400to600_2018', 'WJetsHT600to800_2018', 'WJetsHT800to1200_2018', 'WJetsHT1200to2500_2018', 'WJetsHT2500toInf_2018', 'TprimeToTZ_700_2018', 'TprimeToTZ_1000_2018', 'TprimeToTZ_1800_2018', 'DataHT_2018', 'DataHTA_2018', 'DataHTB_2018', 'DataHTC_2018', 'DataHT_2022', 'DataHTC_2022', 'DataHTD_2022', 'DataHTE_2022', 'DataHTF_2022', 'DataHTG_2022', 'DataHTD_2018'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f503d",
   "metadata": {},
   "source": [
    "Variables to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "616688c8-eb99-452d-b66c-cd938fcc2877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MET_pt', 'PuppiMET_pt', 'MET_phi', 'LeadingJet_pt', 'LeadingFatJet_pt', 'nTopHighPt', 'nTopLowPt', 'nJet', 'nJetBtag', 'nFatJet', 'MinDelta_phi', 'MaxEta_jet', 'HT_eventHT', 'run', 'PV_npvsGood']\n"
     ]
    }
   ],
   "source": [
    "#Defining variables to plot\n",
    "\n",
    "var = vars  # ---> vedi variables.py\n",
    "\n",
    "# var.append(variable(name = \"MET_pt\", title= \"MET p_{T} [GeV]\", taglio = cut, nbins = 20, xmin = 0, xmax=1000))\n",
    "# var.append(variable(name = \"MET_phi\", title= \"MET #phi\", taglio = cut, nbins = 6, xmin = -math.pi, xmax=math.pi))\n",
    "# var.append(variable(name = \"LeadingJet_pt\", title= \"Leading Jet p_{T} [GeV]\", taglio = cut, nbins = 30, xmin = 50, xmax=950))\n",
    "# var.append(variable(name = \"nTopHighPt\", title= \"# Top Candidate Mix\", taglio = cut, nbins = 80, xmin = -0.5, xmax=80.5))\n",
    "# var.append(variable(name = \"nTopLowPt\", title= \"# Top Candidate Resolved\", taglio = cut, nbins = 50, xmin = -0.5, xmax=50.5))\n",
    "# var.append(variable(name = \"nJet\", title= \"# Jet\", taglio = cut, nbins = 25, xmin = -0.5, xmax=25.5))\n",
    "# var.append(variable(name = \"nFatJet\", title= \"# FatJet\", taglio = cut, nbins = 25, xmin = -0.5, xmax=25.5))\n",
    "# var.append(variable(name = \"MinDelta_phi\", title= \"min #Delta #phi\", taglio = cut, nbins = 20, xmin = 0, xmax = 4))\n",
    "# var.append(variable(name = \"MaxEta_jet\", title= \"max #eta jet\", taglio = cut, nbins = 24, xmin = 0, xmax = 6))\n",
    "\n",
    "print([v._name for v in var])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "963af90a-de0c-44a7-9b64-3927c34eeffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### utils ###################\n",
    "def cut_string(cut):\n",
    "    return cut.replace(\" \", \"\").replace(\"&&\",\"_\").replace(\">\",\"_g_\").replace(\".\",\"_\").replace(\"==\",\"_e_\")\n",
    "\n",
    "################### preselection ###############\n",
    "def preselection(df, cut):\n",
    "    if 'leptonveto' in cut:\n",
    "        df_presel = df.Filter(\"LepVeto(Electron_pt, Electron_eta, Electron_cutBased, Muon_pt, Muon_eta, Muon_looseId )\", \"Lepton Veto\")\n",
    "        if \"&& leptonveto\" in cut:\n",
    "            c_ = cut.replace(\"&& leptonveto\",\"\")\n",
    "        elif \"leptonveto &&\" in cut:\n",
    "            c_ = cut.replace(\"leptonveto &&\",\"\")\n",
    "        elif \"leptonveto\" in cut:\n",
    "            c_ = cut.replace(\"leptonveto\",\"\")    \n",
    "    else: \n",
    "        df_presel = df_presel\n",
    "        c_ = cut\n",
    "    df_presel = df_presel.Filter(\"atLeast1verygoodjet(GoodJet_idx, Jet_pt, Jet_eta)\", \"At least 1Ak4 pt>40 eta<4\")\n",
    "    df_presel = df_presel.Filter(\"atLeast1verygoodfatjet(FatJet_pt, FatJet_msoftdrop)\", \"at least 1Ak8 pt>200 msoftdrop>40\")\n",
    "    return df_presel, c_\n",
    "\n",
    "def global_var(df):\n",
    "    df_goodjet = df.Define(\"GoodJet_idx\", \"GetGoodJet(Jet_pt, Jet_jetId)\")\n",
    "    df_leadingjet = df_goodjet.Define(\"LeadingJet_pt\", \"LeadingJetPt(GoodJet_idx, Jet_pt)\")\n",
    "    df_leadingjet = df_leadingjet.Define(\"LeadingFatJet_pt\", \"LeadingFatJetPt(FatJet_pt)\")\n",
    "    df_frwjet = df_leadingjet.Define(\"nForwardJet\", \"nForwardJet(GoodJet_idx, Jet_eta)\")\n",
    "    df_bjet = df_frwjet.Define(\"nJetBtag\", \"njetbtag(GoodJet_idx, Jet_btagDeepFlavB)\")\n",
    "\n",
    "    return df_bjet\n",
    "############### trigger selection #####################\n",
    "def trigger_filter(df):\n",
    "    #HLT_PFHT780 || HLT_PFHT890 || HLT_Mu50 || HLT_OldMu100 || HLT_TkMu100 || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_Ele35_WPTight_Gsf\n",
    "    #preso da MET_HLT_filter\n",
    "    #df_trig = df.Filter(\"HLT_PFHT780 || HLT_PFHT890 || HLT_Mu50 || HLT_OldMu100 || HLT_TkMu100 || HLT_Ele115_CaloIdVT_GsfTrkIdT || HLT_Photon200 || HLT_Ele35_WPTight_Gsf\")\n",
    "    df_trig = df.Filter(\" HLT_PFMET120_PFMHT120_IDTight || HLT_PFMETNoMu120_PFMHTNoMu120_IDTight\", \"trigger\")\n",
    "    # controllare la lista di trigger \n",
    "    return df_trig\n",
    "\n",
    "############### top selection ########################\n",
    "def select_top(df):\n",
    "    df_goodtopMer = df.Define(\"GoodTopMer_idx\", \"select_TopMer(FatJet_deepTag_TvsQCD, FatJet_pt, FatJet_eta, FatJet_phi)\")\n",
    "    # ritorna gli indici dei FatJet che superano la trs del Top Merged (no overlap)\n",
    "    df_goodtopMix = df_goodtopMer.Define(\"GoodTopMix_idx\", \"select_TopMix(TopHighPt_score2, TopHighPt_pt, TopHighPt_eta, TopHighPt_phi)\")\n",
    "    # ritorna gli indici dei FatJet che superano la trs del Top Merged (no overlap)\n",
    "    df_goodtopRes = df_goodtopMix.Define(\"GoodTopRes_idx\", \"select_TopRes(TopLowPt_scoreDNN, TopLowPt_pt, TopLowPt_eta, TopLowPt_phi)\")\n",
    "    # ritorna gli indici dei Fatche superano la trs del Top Merged (no overlap)\n",
    "    df_topcategory = df_goodtopRes.Define(\"EventTopCategory\", \"select_TopCategory(GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx)\")\n",
    "    # return:  0- no top sel, 1- top merged, 2- top mix, 3- top resolved\n",
    "    df_topselected = df_topcategory.Define(\"Top_idx\",\n",
    "                                           \"select_bestTop(EventTopCategory, GoodTopMer_idx, GoodTopMix_idx, GoodTopRes_idx, FatJet_deepTag_TvsQCD, TopHighPt_score2, TopLowPt_scoreDNN)\")\n",
    "    # return best top idx wrt category --> the idx is referred to the list of candidates fixed by the EventTopCategory\n",
    "    df_topvariables = df_topselected.Define(\"Top_pt\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_pt, TopHighPt_pt, TopLowPt_pt)\")\\\n",
    "                        .Define(\"Top_eta\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_eta, TopHighPt_eta, TopLowPt_eta)\")\\\n",
    "                        .Define(\"Top_phi\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_phi, TopHighPt_phi, TopLowPt_phi)\")\\\n",
    "                        .Define(\"Top_mass\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_mass, TopHighPt_mass, TopLowPt_mass)\")\\\n",
    "                        .Define(\"Top_score\", \"select_TopVar(EventTopCategory, Top_idx, FatJet_deepTag_TvsQCD, TopHighPt_score2, TopLowPt_scoreDNN)\")\n",
    "\n",
    "    return df_topvariables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f0bf61-b2ab-422e-8baf-9abba2b984cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookhisto(df, regions_def, var, s_cut):\n",
    "    h_ = {}\n",
    "    for reg in regions_def.keys():\n",
    "        h_[reg] = {}\n",
    "        for v in var:\n",
    "            # print(v._name+\"_plot\", \"overflowBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\". ,\"+str(v._xmax)+\". )\")\n",
    "            if regions_def[reg] == \"\":\n",
    "                h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                # h_[reg][v._name]= df.Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "            else:\n",
    "                h_[reg][v._name]= df.Filter(regions_def[reg], \"Met filter\").Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name, \"w_nominal\")\n",
    "                # h_[reg][v._name]= df.Filter(regions_def[reg], \"Met filter\").Redefine(v._name, \"UnOvBin(\"+v._name+\",\"+str(v._nbins)+\",\"+str(v._xmin)+\",\"+str(v._xmax)+\")\").Histo1D((v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax), v._name)\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37967949-f502-4c77-9ceb-dd04f8062cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def savehisto(d, regions_def, var, s_cut):\n",
    "    histo = {}\n",
    "    histo[d.label] = {reg: {v._name: ROOT.TH1D(v._name+\"_\"+reg+\"_\"+s_cut,\" ;\"+v._title+\"\", v._nbins, v._xmin, v._xmax) for v in var} for reg in regions_def.keys()}\n",
    "    if recreate_files== True:\n",
    "        outfile = ROOT.TFile.Open(repohisto+d.label+'.root', \"RECREATE\")\n",
    "    else:\n",
    "        outfile = ROOT.TFile.Open(repohisto+d.label+'.root', \"Update\")\n",
    "    for reg in regions_def.keys():\n",
    "        for v in var:\n",
    "            if hasattr(d, \"components\"):\n",
    "                s_list = d.components\n",
    "                for s in s_list:\n",
    "                    \n",
    "                    # if not os.path.exists(repohisto+s.label+'.root'):\n",
    "                    #     outfile_s = ROOT.TFile.Open(repohisto+s.label+'.root', \"RECREATE\")\n",
    "                    # else:\n",
    "                    #     outfile_s = ROOT.TFile.Open(repohisto+s.label+'.root', \"UPDATE\")\n",
    "                    tmp = h[d.label][s.label][reg][v._name].GetValue()\n",
    "                    if not 'Data' in s.label:\n",
    "                        ntot = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "                        w = sample_dict[s.label].sigma*lumi*10**3/ntot\n",
    "                        tmp.Scale(w)  \n",
    "                    # outfile_s.cd()\n",
    "                    # tmp.Write()\n",
    "                    histo[d.label][reg][v._name].Add(tmp)\n",
    "            else:\n",
    "                s_list = [d]\n",
    "                for s in s_list:\n",
    "                    # print(\"saving \"+s.label+\" \"+ reg + \" \"+ v._name)\n",
    "                    tmp = h[d.label][s.label][reg][v._name].GetValue() \n",
    "                    if not 'Data' in s.label:\n",
    "                        ntot = np.sum(samples[d.label][s.label]['ntot'][:nfiles])\n",
    "                        w = sample_dict[s.label].sigma*lumi*10**3/ntot\n",
    "                        tmp.Scale(w)\n",
    "                    histo[d.label][reg][v._name].Add(tmp)\n",
    "            # print(\"saved \" + reg +\" for all samples !\")\n",
    "            outfile.cd()\n",
    "            histo[d.label][reg][v._name].Write()\n",
    "    outfile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54c96d28-45f4-4877-8d00-680e095a3016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#samples['DataHTF_2022']['DataHTF_2022']['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab036200-65f9-44a0-abbb-8a04003fc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples[d.label][d.components[0]]['strings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b72ddfd-d641-437d-a78c-8568a0a17a30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop on datasets:  ['DataHT_2018']\n",
      "Local time : 2023-11-09 11:56:04.089106\n",
      "requirements: leptonveto\n",
      "Initializing DataFrame for DataHTA_2018 chain len =  699\n",
      "Initializing DataFrame for DataHTB_2018 chain len =  328\n",
      "Initializing DataFrame for DataHTC_2018 chain len =  331\n",
      "All histos booked !\n",
      "DataHT_2018 finished!\n",
      "Job finished in:  0:33:36.895850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TClass::Init>: no dictionary for class edm::Hash<1> is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ParameterSetBlob is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessHistory is available\n",
      "Warning in <TClass::Init>: no dictionary for class edm::ProcessConfiguration is available\n",
      "Warning in <TClass::Init>: no dictionary for class pair<edm::Hash<1>,edm::ParameterSetBlob> is available\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "print(\"starting loop on datasets: \",[d.label for d in datasets])\n",
    "print(\"Local time :\", t0)\n",
    "print(\"requirements: \"+cut)\n",
    "h = {}\n",
    "reports = {}\n",
    "\n",
    "for d in datasets:\n",
    "    h[d.label] = {}\n",
    "    reports[d.label]= {}\n",
    "    # print(d.label)\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    for s in s_list:\n",
    "        if 'Data' in s.label : isMC = False\n",
    "        else: isMC = True\n",
    "        c_ = cut\n",
    "        \n",
    "        if nfiles_max > len(samples[d.label][s.label]['strings']): \n",
    "            nfiles = len(samples[d.label][s.label]['strings'])\n",
    "            for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "                samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain = samples[d.label][s.label]['strings']\n",
    "        else: \n",
    "            nfiles = nfiles_max\n",
    "            # for i, string in enumerate(samples[d.label][s.label]['strings']): \n",
    "            #     samples[d.label][s.label]['strings'][i] = string.replace(\"root://cms-xrd-global.cern.ch/\", \"root://stormgf2.pi.infn.it/\")\n",
    "            chain = samples[d.label][s.label]['strings'][:nfiles]\n",
    "        print(\"Initializing DataFrame for \"+ s.label +\" chain len = \", len(chain))\n",
    "        if len(chain)==1: print(chain)\n",
    "        if distributed ==True:\n",
    "            df = RDataFrame(\"Events\", chain, npartitions=nmaxpartition, \n",
    "                            daskclient=client, monitor_label = \"main\" )\n",
    "        else:\n",
    "            df = RDataFrame(\"Events\", chain)\n",
    "            \n",
    "        df = df.Define(\"HEMVeto\", \"hemveto(Jet_eta, Jet_phi, Electron_eta, Electron_phi)\")\n",
    "        df = df.Define('w_nominal', '1')\n",
    "        if s.year == 2018: \n",
    "            if not isMC:\n",
    "                df = df.Filter('HEMVeto || run<319077.', \"HEM Veto\")\n",
    "            elif isMC: # else\n",
    "                df = df.Redefine('w_nominal', 'w_nominalhemveto(w_nominal, HEMVeto)')\n",
    "        \n",
    "        if 'TT' in s.label and not 'Mtt' in s.label:\n",
    "            df = df.Filter(\"tt_mtt_doublecounting(GenPart_pdgId, GenPart_pt, GenPart_eta, GenPart_phi, GenPart_mass)\", \"TT double counting\")\n",
    "        \n",
    "        # df = df.Filter(\"MET_pt>200\", \"MET > 200\") ####\n",
    "        \n",
    "        df = df.Filter(\"MET_HLT_filter(HLT_PFMET120_PFMHT120_IDTight, HLT_PFMETNoMu120_PFMHTNoMu120_IDTight, Flag_goodVertices, Flag_globalSuperTightHalo2016Filter, Flag_HBHENoiseFilter, Flag_HBHENoiseIsoFilter, Flag_EcalDeadCellTriggerPrimitiveFilter, Flag_BadPFMuonFilter, Flag_ecalBadCalibFilter, Flag_eeBadScFilter)\", \"HLT_MET_filter\")\n",
    "        # df_trigger = trigger_filter(df)\n",
    "        df_globalvar = global_var(df)\n",
    "        df_presel, c_ = preselection(df_globalvar, cut)\n",
    "        \n",
    "        if top_selection: \n",
    "            df_seltop = select_top(df_presel) \n",
    "            df_mt = df_seltop.Define(\"Mt\", \"sqrt(2 * Top_pt * MET_pt * (1 - cos(Top_phi - MET_phi)))\") \n",
    "        else:\n",
    "            df_mt = df_presel\n",
    "            \n",
    "        if c_==\"\": df_plot = df_mt#df_lepveto\n",
    "        else: df_plot = df_mt.Filter(c_, \"others\")\n",
    "            \n",
    "        h[d.label][s.label] = {}\n",
    "        s_cut = cut_string(cut)\n",
    "        tmp = bookhisto(df_plot, regions_def, var, s_cut)\n",
    "        h[d.label][s.label] = tmp\n",
    "        if not distributed:\n",
    "            df_report = df_plot.Filter(\"MET_pt>200\", \"MET pt cut (200)\") \n",
    "            reports[d.label][s.label] = df_report.Report().Print()\n",
    "        \n",
    "print(\"All histos booked !\")\n",
    "for d in datasets:\n",
    "    savehisto(d, regions_def, var, s_cut)\n",
    "    print(d.label + \" finished!\")\n",
    "t1 = datetime.now()\n",
    "print(\"Job finished in: \", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9573b747-9a5a-4501-b084-481b10d418ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataHT_2018 DataHTA_2018\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DataHTA_2018'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m s_list:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(d\u001b[38;5;241m.\u001b[39mlabel, s\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mreports\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'DataHTA_2018'"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    if hasattr(d, \"components\"):\n",
    "        s_list = d.components\n",
    "    else:\n",
    "        s_list = [d]\n",
    "    for s in s_list:\n",
    "        print(d.label, s.label)\n",
    "        reports[d.label][s.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269963fc-638b-4a19-8c8d-af26bad6e37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity kernel",
   "language": "python",
   "name": "singularity-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
